---
title: "ISYE6501_WK1_Homework"
output: html_notebook
---

**Question 2.1** Describe a situation or problem from your job, everyday life, current events, etc., for which a classification model would be appropriate. List some (up to 5) predictors that you might use.

**Answer:**

We can describe my likelihood to buy a new houseplant with using a classification model.

For example the question to be answered would be "Will I buy this plant?" and a short list of predictors would be:

1.  Care level of the plant (Categorical, Easy/Medium/Difficult): Many houseplants have a simple indicator of how difficult it is to care for. This takes into consideration many other factors such as how likely it is to get root rot, how likely it is to get pests, how resistant it is to environmental changes such as changes in temperature, humidity, watering schedule, etc. Simply put: easy plants are hard to kill.
2.  Optimal Light Requirements (Continuous, in units FC): Plants can tolerate a range of light but do actually have an empirically determined optimal amount of light that they require.
3.  Bank Account Balance (Continuous, in units \$): I will buy a plant if I have cash to spare.
4.  Have I bought a plant recently (Binary, Yes/No): In general, if I have bought a plant recently I will not buy another plant.

**Question 2.2**

The files credit_card_data.txt (without headers) and credit_card_data-headers.txt (with headers) contain a dataset with 654 data points, 6 continuous and 4 binary predictor variables.  It has anonymized credit card applications with a binary response variable (last column) indicating if the application was positive or negative. The dataset is the “Credit Approval Data Set” from the UCI Machine Learning Repository (<https://archive.ics.uci.edu/ml/datasets/Credit+Approval>) without the categorical variables and without data points that have missing values.

1.  Using the support vector machine function `ksvm` contained in the R package `kernlab`, find a good classifier for this data. Show the equation of your classifier, and how well it classifies the data points in the full data set.  (Don’t worry about test/validation data yet; we’ll cover that topic soon.)

```{r}
# load data
df = read.delim('week 1 data-summer/data 2.2/credit_card_data-headers.txt')
head(dataframe, 5)
```

```{r}
# import packages
#install.packages('kernlab')
library(kernlab)

# building a SVM with kernlab
model = ksvm(as.matrix(dataframe[,1:10]),
             as.factor(dataframe[,11]),
             type="C-svc",
             kernel="vanilladot",
             C=100,
             scaled=T)

# calculate a1…am
a = colSums(model@xmatrix[[1]] * model@coef[[1]])
# calculate a0
a0 = -model@b
# see what the model predicts
pred = predict(model, dataframe[,1:10])
```

```{r}
# print coefficients of the equation
a
```

```{r}
# print y-intercept
a0
```

```{r}
# print accuracy
# see what fraction of the model’s predictions match the actual classification
sum(pred == dataframe[,11]) / nrow(dataframe)
```

```{r}
# Keeping track:
# C = 1000
# C = 295, Accuracy = 0.8623853
# C = 100, Accuracy = 0.8639144
# C = 10
# C = 1
# C = 0.1
# C = 0.01
# C = 0.001, Accuracy = 0.8379205
# C = 0.0001, Accuracy = 0.5474006
```

**Q2.2 1) Answer:**

-   The optimal value of C seems to be around 0.01 to 100

-   The equation of the classifier is:

    -   $$
        -7.986420e^{-06}x_{1} - 5.463295e^{-04}x_{2} - 2.279577e^{-04}x_{3} - 4.433986e^{-05}x_{4} + 9.982332e^{-01}x_{5} - 6.768640e^{-05}x_{6} + 2.472771e^{-04}x_{7} + 2.411323e^{-04}x_{8} + 1.721389e^{-04}x_{9} + 3.157305e^{-04}x_{10} + 0.07040592 = 0
        $$

-   The SVM classifies the data points in the full dataset with an accuracy of

    -   0.8639144

2.  You are welcome, but not required, to try other (nonlinear) kernels as well; we’re not covering them in this course, but they can sometimes be useful and might provide better predictions than `vanilladot`

```{r}
# iterate over different types of kernels
# polydot kernel
model2 = ksvm(as.matrix(dataframe[,1:10]),
              as.factor(dataframe[,11]),
              type="C-svc",
              kernel="polydot",
              C=100,
              scaled=T)

# see what the model predicts
pred = predict(model2,dataframe[,1:10])

# see what fraction of the model’s predictions match the actual classification
sum(pred == dataframe[,11]) / nrow(dataframe)
```

```{r}
# laplacedot kernel
model3 = ksvm(as.matrix(dataframe[,1:10]),
              as.factor(dataframe[,11]),
              type="C-svc",
              kernel="laplacedot",
              C=100,
              scaled=T)

# see what the model predicts
pred = predict(model3,dataframe[,1:10])

# see what fraction of the model’s predictions match the actual classification
sum(pred == dataframe[,11]) / nrow(dataframe)
```

```{r}
# laplacedot kernel
model4 = ksvm(as.matrix(dataframe[,1:10]),
              as.factor(dataframe[,11]),
              type="C-svc",
              kernel="besseldot",
              C=100,
              scaled=T)

# see what the model predicts
pred = predict(model4,dataframe[,1:10])

# see what fraction of the model’s predictions match the actual classification
sum(pred == dataframe[,11]) / nrow(dataframe)
```

3.   Using the k-nearest-neighbors classification function kknn contained in the R `kknn` package,
    suggest a good value of k, and show how well it classifies that data points in the full data set.
    Don’t forget to scale the data (`scale=TRUE in kknn`).

```{r}
# import package
#install.packages('kknn')
library(kknn)

# build model
model = kknn(formula=formula(), 
             scale=T)
```
